{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f1faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler , OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e3dff4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/cillian/Desktop/IronHack-Mid-project./data_mid_bootcamp_project_regression/data_mid_bootcamp_project_regression/regression_data.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m relative_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_mid_bootcamp_project_regression/regression_data.xls\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_directory, relative_path)\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m df\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/excel/_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    477\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/excel/_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1496\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1501\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1502\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1503\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/excel/_base.py:1371\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1369\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1374\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1375\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/cillian/Desktop/IronHack-Mid-project./data_mid_bootcamp_project_regression/data_mid_bootcamp_project_regression/regression_data.xls'"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "relative_path = 'data_mid_bootcamp_project_regression/regression_data.xls'\n",
    "file_path = os.path.join(current_directory, relative_path)\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114ef66",
   "metadata": {},
   "source": [
    "## 1.1 Initial look at Dataset and values, checking for nulls or incomplete sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d73cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75726e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for nulls.\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3eafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ec6d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All of our data is complete and no null values or strange values found in value count.\n",
    "for col in df.columns:\n",
    "    print(df[col].value_counts(), '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b9fea0",
   "metadata": {},
   "source": [
    "## 1.2 Formatting the dataset types\n",
    "- We have columns that are catergorical in nature but are represented as numerical.\n",
    "- Bathrooms, Bedrooms, Floors, Waterfront, View, Condition and Grade are seen as categorical here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c9aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all columns will be turned into 'str' apart form waterfront which will be turned into 'bool'\n",
    "\n",
    "def format_columns(df):\n",
    "    columns_to_str = ['bathrooms', 'bedrooms', 'floors', 'view', 'condition', 'grade', 'zipcode']\n",
    "    columns_to_bool = ['waterfront']\n",
    "    \n",
    "    for col in columns_to_str:\n",
    "        df[col] = df[col].astype(str)\n",
    "        \n",
    "    for col in columns_to_bool:\n",
    "        df[col] = df[col].astype(bool)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df = format_columns(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placing the id as index as it is not needed for our model but is still usefull in dataset\n",
    "# also dropping date\n",
    "df = df.set_index('id')\n",
    "df = df.drop('date', axis = 1,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8e0d6",
   "metadata": {},
   "source": [
    "#### There is potential to bucket or engineer yr_renovated and zipcode into categorical. But happy from here to check distribution. and correlations.\n",
    "- yr_renovated into Boolean?\n",
    "- Zipcode bin into high and low price to represent areas maybe by price/sqft?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501d8c8",
   "metadata": {},
   "source": [
    "## 2.1 EDA (Distribution,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading numerical and categorical columns into variables.\n",
    "num_df = df.select_dtypes(include='number')\n",
    "cat_df = df.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b09cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking distribution for the numerical columns\n",
    "for col in num_df.columns:\n",
    "    fig =plt.figure(figsize=(12,6))\n",
    "    sns.distplot(num_df[col], hist=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d1a90",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "- The columns that represent Sqft skew to the right which is normal when considering that some houses will be bigger, canadate for log scaling or normalisation, apart from sqft_basement which has a lot of zeros meaning a lot of houses dont have basements.\n",
    "- The target feature price also sees a skew to the right which is normal with monetary values.\n",
    "- Rest are normal considering represtation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fc4d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the categorical\n",
    "# variable is used to excudle the zipcode graph as it is unreadabl\n",
    "\n",
    "cat_nozip = cat_df.columns[:-1]\n",
    "\n",
    "for col in cat_nozip:\n",
    "    plt.title(f'Countplot for {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('price')\n",
    "    plt.xticks(rotation = 45)\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    sorted_df = df.sort_values(by=col)\n",
    "    sns.countplot(x=col, data=df)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642ba6f",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "- 3 or 4 bedroom houses are the avg sold.\n",
    "- Interesting to see the different characteristics of the avg house in our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230f037",
   "metadata": {},
   "source": [
    "## 2.2 EDA (Correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix \n",
    "corr_matrix = df.corr(method= 'pearson')\n",
    "\n",
    "mask = np.zeros_like(corr_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (15,11))\n",
    "ax = sns.heatmap(corr_matrix, mask = mask, annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920808b",
   "metadata": {},
   "source": [
    "#### Results of matrix.\n",
    "- Sqft_above and sqft_living show a very high correlation of 0.88. This makes sense when you think they are essentially the same info with sqft_above refering to the sqft of the house apart of the basement and sqft_living refering to above + basement.\n",
    "- With this observation Sqft_above and Sqft_basement can be dropped and the same info still present in pur data.\n",
    "- Considering the high correlation between sqft_lot15 and sqft_lot, sqft_living15, sqft_living and there similar meanings seeing if we can drop one of each will be looked into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22976b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqft_drop = ['sqft_above', 'sqft_basement']\n",
    "df = df.drop(sqft_drop , axis=1,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bbd6bf",
   "metadata": {},
   "source": [
    "## 2.3 EDA (Outliers)\n",
    "- with dealing with the housing market there is bound to be a fair share of outliers in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12780f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First checking outliers with our target feature 'price'\n",
    "sns.boxplot(y='price', data=df, orient='v')\n",
    "plt.title('Boxplot for Price')\n",
    "plt.ylabel('Price')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88676a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Catergorical Columns\n",
    "# cat_nozip is used again to ignore zipcode column.\n",
    "\n",
    "\n",
    "for col in cat_nozip:\n",
    "    plt.title(f'Boxplot for {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('price')\n",
    "    plt.xticks(rotation = 45)\n",
    "    sns.boxplot(x=col, y='price', data =df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numerical columns.\n",
    "df.describe()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e31b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redfining num_df after drop sqft_above and sqft_basement.\n",
    "num_df = df.select_dtypes(include='number')\n",
    "for col in num_df:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    num_outliers = len(outliers)\n",
    "    print(f'Outliers in {col}: {num_outliers} outliers')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a743dca",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "- As we can see we have a far amount of outliers across the board which is normal when considering the many factors affecting our features.\n",
    "- I think removing outliers in our model may be a bad idea as we will always have outliers in a housing market but scaling may help our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34b65c",
   "metadata": {},
   "source": [
    "### Here is a good point to get out benchmark models before we deep dive further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827547e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reg_models = [LinearRegression(), KNeighborsRegressor(), MLPRegressor()]\n",
    "# Splitting data into train and test\n",
    "X = pd.get_dummies(df.drop('price', axis=1))\n",
    "y = df['price']\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)\n",
    "\n",
    "\n",
    "def regression_model(models, data):\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        score = model.score(X_test, y_test)\n",
    "        rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, pred) * 100\n",
    "        print(\"Model:\", model.__class__.__name__)\n",
    "        print(\"R2_score:\", round(score, 2))\n",
    "        print(\"RMSE:\", round(rmse, 2))\n",
    "        print(\"MAE:\", round(mae, 2))\n",
    "        print(\"MAPE:\", round(mape, 2), \"%\")\n",
    "        print()\n",
    "\n",
    "        # Scatter plot of actual vs. predicted values\n",
    "        sns.regplot(x=y_test, y=pred)\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title('Actual vs. Predicted Values')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "regression_model(reg_models, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba67e1",
   "metadata": {},
   "source": [
    "#### Results\n",
    "- So far our data is best fit for LinearRegression with an R2 of 0.79 and the worst is KNeighboursRegressor with an R2 score of 0.45.\n",
    "- But our Rmse and mae of our best model is quite high 163,649 and 97,348 respectively.\n",
    "- Our plots show the effect of outliers on our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5345d79",
   "metadata": {},
   "source": [
    "## 3: Feature Engineering and Selection\n",
    "- Looking at Sqft living/lot V sqft living15/lot15 to see differences and maybe dropping\n",
    "- yr_renovated change to yes or no?\n",
    "- Scaling.\n",
    "- Feature Importance. VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca06db",
   "metadata": {},
   "source": [
    "#### 3.1:  Sqft column analysis.\n",
    "- Comparing sqft_living with sqft_living15 and sqft_lot with sqft_lot15.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a69a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a series by comparing the values of each column and then representing the proportion as a percentage.\n",
    "comparison_living = df['sqft_living15'] >= df['sqft_living']\n",
    "comparison_living.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca40a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_living = df['sqft_lot15'] >= df['sqft_lot']\n",
    "comparison_living.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2b02b",
   "metadata": {},
   "source": [
    "- The definition of these features implies renovations to the house this would mean usually the size gets bigger so I seen what percentage of each column this was true in. Now again not all renovations means the size increases but maybe an extra bath or bedroom is added so for the results to show over 50% and the high correlation shown, I think its okay to make the judgement to drop these columns based on containing similar info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97053011",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sqft_drop = ['sqft_living', 'sqft_lot']\n",
    "#df = df.drop(sqft_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eff8214",
   "metadata": {},
   "source": [
    "- After comparing the ML models with and without these columns we seen marginally better results including these columns so I will leave them in our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610135cd",
   "metadata": {},
   "source": [
    "#### 3.2: yr_renovated\n",
    "- As most of our values for yr_renovated are 0 or not renovated i think this column would be better represented as Yes or No. So i will convert it into a boolean type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149373e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['yr_renovated'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting using lambda function where 0 is False and else is True.\n",
    "df['yr_renovated'] = df['yr_renovated'].map(lambda x: False if x == 0 else True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75272a",
   "metadata": {},
   "source": [
    "### 3.3: Scaling\n",
    "- Two Methods of scaling will be tested for our model. Standard Scaler and log scaling.\n",
    "- For log scaling it will not take the longitude column as it contains negative values so we will remove both lat and long columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ecfff",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8fd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model_scaled (models, data):\n",
    "    X = data.drop('price', axis=1)\n",
    "    y = data['price']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        pred = model.predict(X_test_scaled)\n",
    "        score = model.score(X_test_scaled, y_test)\n",
    "        rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, pred) * 100\n",
    "        print(\"Model:\", model.__class__.__name__)\n",
    "        print(\"R2_score:\", round(score, 2))\n",
    "        print(\"RMSE:\", round(rmse, 2))\n",
    "        print(\"MAE:\", round(mae, 2))\n",
    "        print(\"MAPE:\", round(mape, 2), \"%\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "        # Scatter plot of actual vs. predicted values\n",
    "        sns.regplot(x=y_test, y=pred)\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title('Actual vs. Predicted Values')\n",
    "        plt.show()\n",
    "\n",
    "regression_model_scaled(reg_models, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806246a",
   "metadata": {},
   "source": [
    "#### Log Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model_log(models, data):\n",
    "    # Dropping 'long' and 'lat' columns\n",
    "    long_lat = ['long', 'lat']\n",
    "    data = data.drop(long_lat, axis=1)\n",
    "\n",
    "    # Splitting the data into train and test sets\n",
    "    X = pd.get_dummies(data.drop('price', axis=1))\n",
    "    y = data['price']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)\n",
    "\n",
    "    # Log transformation of numerical columns for the training set only\n",
    "    num_df = X_train.select_dtypes(include='number').columns\n",
    "    X_train[num_df] = np.log1p(X_train[num_df])\n",
    "\n",
    "    # Log transformation of target variable for train and test sets\n",
    "    y_train_log = np.log1p(y_train)\n",
    "    y_test_log = np.log1p(y_test)\n",
    "\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train_log)\n",
    "        # Apply log transformation to the test set using the same parameters as the training set\n",
    "        X_test_transformed = X_test.copy()\n",
    "        X_test_transformed[num_df] = np.log1p(X_test_transformed[num_df])\n",
    "\n",
    "        pred_log = model.predict(X_test_transformed)\n",
    "        pred = np.expm1(pred_log)  # Back-transform to the original scale\n",
    "\n",
    "        # Calculate evaluation metrics on the original scale\n",
    "        score = model.score(X_test_transformed, y_test_log)\n",
    "        rmse = mean_squared_error(np.expm1(y_test_log), pred, squared=False)\n",
    "        mae = mean_absolute_error(np.expm1(y_test_log), pred)\n",
    "        mape = mean_absolute_percentage_error(np.expm1(y_test_log), pred) * 100\n",
    "\n",
    "        print(\"Model:\", model.__class__.__name__)\n",
    "        print(\"R2_score:\", round(score, 2))\n",
    "        print(\"RMSE:\", round(rmse, 2))\n",
    "        print(\"MAE:\", round(mae, 2))\n",
    "        print(\"MAPE:\", round(mape, 2), \"%\")\n",
    "        print()\n",
    "\n",
    "        # Scatter plot of actual vs. predicted values\n",
    "        sns.regplot(x=np.expm1(y_test_log), y=pred)\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title('Actual vs. Predicted Values')\n",
    "        plt.show()\n",
    "\n",
    "regression_model_log(reg_models, df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f05ed9",
   "metadata": {},
   "source": [
    "### Results:\n",
    "After applying both Standard Scaler and log transforming separately we can compare the results.\n",
    "- LinearRegression in both models did not perform well with the R2 staying the same with StdScaler and losing 6 with log.\n",
    "- However KNeighborsRegressor nearly doubled in both forms of scaling with the highest score .81 with StdScaler and a  RMSE of 158,670.65 and a MAE of 81,336.86\n",
    "-  MLPRegressor negatively performed with scaling with its highest R2 score at 0.08 with log scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c9563",
   "metadata": {},
   "source": [
    "### 3.4: Feature Importance using VIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ab027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummied = pd.get_dummies(df)\n",
    "df_dummied.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "\n",
    "vif = add_constant(df_dummied)\n",
    "\n",
    "# Step 1: Check for missing values\n",
    "if vif.isnull().values.any():\n",
    "    raise ValueError(\"DataFrame contains missing values. Please handle them first.\")\n",
    "\n",
    "# Step 2: Check for infinite values for numeric columns only\n",
    "numeric_columns = vif.select_dtypes(include=[np.number]).columns\n",
    "if not np.isfinite(vif[numeric_columns].values).all():\n",
    "    raise ValueError(\"DataFrame contains non-finite (e.g., infinity or NaN) values. Please handle them first.\")\n",
    "\n",
    "# Step 3: Calculate VIF for numeric columns only\n",
    "threshold = 10\n",
    "\n",
    "while True:\n",
    "    # Calculate VIF for each numeric column (excluding the constant column)\n",
    "    values = [variance_inflation_factor(vif[numeric_columns].values, i)\n",
    "              for i in range(1, len(numeric_columns))]\n",
    "    # Display VIF values for each column\n",
    "    display(pd.DataFrame(values, index=numeric_columns[1:]).sort_values(0))\n",
    "    if max(values) > threshold:\n",
    "        col_index = values.index(max(values)) + 1\n",
    "        column_name = numeric_columns[col_index]\n",
    "        vif = vif.drop(column_name, axis=1)\n",
    "        numeric_columns = vif.select_dtypes(include=[np.number]).columns\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5761ec",
   "metadata": {},
   "source": [
    "#### Results\n",
    "- I encountered some problems trying to calculate the VIF bbut found this work around using only the numeric columns.\n",
    "- The actual results dont show any features with a value of 10. So no multicolinearity is seen to a high level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038eeae",
   "metadata": {},
   "source": [
    "## 4. Relooking at outliers.\n",
    "- although there will always be outliers in the housing market we can try to remove a small amount to improve our model.\n",
    "- We will look at the outliers in our target feature price with IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "IQR = abs(np.quantile(df[\"price\"], 0.25) - np.quantile(df[\"price\"], 0.75)) * 1.5\n",
    "lower_boundary = np.quantile(df[\"price\"], 0.25) - IQR\n",
    "upper_boundary = np.quantile(df[\"price\"], 0.75) + IQR\n",
    "\n",
    "outliers_count = ((df['price'] < lower_boundary) | (df['price'] > upper_boundary)).sum()\n",
    "total_data = len(df)\n",
    "percentage_of_outliers = round((outliers_count / total_data) * 100 ,2)\n",
    "\n",
    "# Outliers for this column are values smaller than lower_boundary or bigger than upper_boundary:\n",
    "print(percentage_of_outliers)\n",
    "print(\"Lower Boundary:\", lower_boundary)\n",
    "print(\"Upper Boundary:\", upper_boundary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3314683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = df[(df[\"price\"] < lower_boundary) | (df[\"price\"] > upper_boundary)].sort_values(\"price\")\n",
    "outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659950b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(df[col].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in outlier_df.columns:\n",
    "    print(outlier_df[col].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25f48e",
   "metadata": {},
   "source": [
    "#### A quick analysis of the value counts of the outliers and the original data.\n",
    "- As expected we see our outliers show higher values in the most wanted features of a house. ie more bedrooms, bathrooms, sqft.\n",
    "- Interesting to see most of our waterfront houses are in high value houses.\n",
    "- also seen in grades past 11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995eabe",
   "metadata": {},
   "source": [
    "#### Now lets see how removing these affects our models.\n",
    "- Again comparing StandardScaling and Log Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_out_df = df[(df[\"price\"] >= lower_boundary) & (df[\"price\"] <= upper_boundary)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0b88d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def regression_model_scaled (models, data):\n",
    "    #Splitting the data\n",
    "    X = data.drop('price', axis=1)\n",
    "    y = data['price']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        pred = model.predict(X_test_scaled)\n",
    "        score = model.score(X_test_scaled, y_test)\n",
    "        rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, pred) * 100\n",
    "        print(\"Model:\", model.__class__.__name__)\n",
    "        print(\"R2_score:\", round(score, 2))\n",
    "        print(\"RMSE:\", round(rmse, 2))\n",
    "        print(\"MAE:\", round(mae, 2))\n",
    "        print(\"MAPE:\", round(mape, 2), \"%\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "        # Scatter plot of actual vs. predicted values\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.regplot(x=y_test, y=pred, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title('Actual vs. Predicted Values')\n",
    "        plt.show()\n",
    "\n",
    "regression_model_scaled(reg_models, no_out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11252343",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def regression_model_log(models, data):\n",
    "    # Dropping 'long' and 'lat' columns\n",
    "    long_lat = ['long', 'lat']\n",
    "    data = data.drop(long_lat, axis=1)\n",
    "\n",
    "    # Splitting the data into train and test sets\n",
    "    X = pd.get_dummies(data.drop('price', axis=1))\n",
    "    y = data['price']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)\n",
    "\n",
    "    # Log transformation of numerical columns for the training set only\n",
    "    num_df = X_train.select_dtypes(include='number').columns\n",
    "    X_train[num_df] = np.log1p(X_train[num_df])\n",
    "\n",
    "    # Log transformation of target variable for train and test sets\n",
    "    y_train_log = np.log1p(y_train)\n",
    "    y_test_log = np.log1p(y_test)\n",
    "\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train_log)\n",
    "        # Apply log transformation to the test set using the same parameters as the training set\n",
    "        X_test_transformed = X_test.copy()\n",
    "        X_test_transformed[num_df] = np.log1p(X_test_transformed[num_df])\n",
    "\n",
    "        pred_log = model.predict(X_test_transformed)\n",
    "        pred = np.expm1(pred_log)  # Back-transform to the original scale\n",
    "\n",
    "        # Calculate evaluation metrics on the original scale\n",
    "        score = model.score(X_test_transformed, y_test_log)\n",
    "        rmse = mean_squared_error(np.expm1(y_test_log), pred, squared=False)\n",
    "        mae = mean_absolute_error(np.expm1(y_test_log), pred)\n",
    "        mape = mean_absolute_percentage_error(np.expm1(y_test_log), pred) * 100\n",
    "\n",
    "        print(\"Model:\", model.__class__.__name__)\n",
    "        print(\"R2_score:\", round(score, 2))\n",
    "        print(\"RMSE:\", round(rmse, 2))\n",
    "        print(\"MAE:\", round(mae, 2))\n",
    "        print(\"MAPE:\", round(mape, 2), \"%\")\n",
    "        print()\n",
    "\n",
    "        # Scatter plot of actual vs. predicted values\n",
    "        sns.regplot(x=np.expm1(y_test_log), y=pred, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title('Actual vs. Predicted Values')\n",
    "        plt.show()\n",
    "\n",
    "regression_model_log(reg_models, no_out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c1831f",
   "metadata": {},
   "source": [
    "### Report and conclusions.\n",
    "After trailing and testing different techniques it is time to compare results and find which techniques suit our data the best for our chosen Machine Learning model.\n",
    "\n",
    "\n",
    "Log scaling without outliers is overall where we see the best results from our chosen models Linear Regression, KNeighborsRegressor and MLPRegressor, however throughout the project MLPRegressor was the least effective.\n",
    "\n",
    "- Linear Regression--------------------KNeighborsRegressor\n",
    "\n",
    " R2_score: 0.82------------------------R2_score: 0.81\n",
    " \n",
    " RMSE: 87023.39------------------------RMSE: 89257.46\n",
    " \n",
    " MAE: 64793.14-------------------------MAE: 62934.13\n",
    " \n",
    " MAPE: 15.35 %-------------------------MAPE: 14.19 %\n",
    "\n",
    "Comparing the two models Linear Regression performs slightly better in terms of R2 score and Rmse with average predictions deviating by 87,023.39. KNeighborsRegressor edges it slightly with MAE at on avergae 64,793.14 or 14.19% off the predictions. These are both good scores and seem to provide reasonable predictions.\n",
    "\n",
    "However with excluding outliers we limit our model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b77f61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d71df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c48cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956bfee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
